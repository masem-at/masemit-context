# CRITICAL BUG: Decentralization Score Unstable Between Generations

**Priority:** BLOCKER
**Affected:** PDF Report Generation + Score Calculation Pipeline
**Reporter:** Mario
**Date:** February 10, 2026

---

## The Problem

The Decentralization Score for the **same DAO (Lido)** on the **same day** produces different results with every PDF generation:

| Generation | Decentralization Score | Letter Grade | Time |
|-----------|----------------------|-------------|------|
| #1 | **4/10** | C | Feb 10, 2026 |
| #2 | **3/10** | D | Feb 10, 2026 |
| #3 | **3/10** | D | Feb 10, 2026 |
| #4 | **3/10** | D | Feb 10, 2026 |
| #5 | **2/10** | F | Feb 10, 2026 |

The score is **drifting downward** with each generation. Meanwhile the underlying data hasn't changed — same 10 proposals, same 170 voters, same power distribution (Gini 0.915, Nakamoto 6, Top 5 = 50.7%).

The GVS (6.9) and all sub-scores (HPR 10.0, DEI 3.7, PDI 4.4, GPI 7.8) remain stable across all generations. **Only the Decentralization Score changes.**

---

## Why This Is Critical

We're about to send this report to Lido DAO governance contributors as a thank-you for their input on the Governance Attention Map. If the score changes every time someone generates a report, our credibility is destroyed.

Worse: if a Lido contributor generates their own report and gets a different score than what we sent them, it looks like we're manipulating results.

---

## Likely Root Causes

### Hypothesis 1: Non-Deterministic AI/LLM Generation
The Decentralization Score text and/or numeric rating might be generated by an LLM call (Claude API) as part of the report generation, with temperature > 0. Each generation produces slightly different interpretations of the same data.

**Check:** Is the Decentralization Score calculated deterministically from data, or is it LLM-generated/interpreted?

**Fix:** If LLM-generated, either:
- Set temperature to 0 for score generation
- Replace LLM interpretation with a deterministic formula
- Pre-calculate the score and pass it as a fixed input to the LLM for narrative generation only

### Hypothesis 2: Boundary Sensitivity
The raw score might sit at a boundary (e.g., 2.5 on a 0-10 scale), and minor floating-point or rounding differences tip it between grades with each generation.

**Check:** Log the raw decimal score before rounding. Is it consistently the same value?

**Fix:** Implement hysteresis/snapping — once a score is calculated, it doesn't change grade unless it moves ≥0.5 points past the boundary. Or show the decimal score (e.g., "2.5/10") to avoid artificial cliff effects.

### Hypothesis 3: Different Data Snapshots
Each generation might pull slightly different data — e.g., new proposals or votes arriving between generations.

**Check:** Are all generations using the exact same 10 proposals and 170 voters? Log the proposal IDs and voter count per generation.

**Fix:** Pin report generation to a specific data snapshot. Add timestamp: "Data snapshot: Feb 10, 2026 00:00 UTC"

### Hypothesis 4: Prompt Variation
The LLM prompt for generating the report might not include a fixed score, so the LLM "decides" the score based on its interpretation of the metrics each time.

**Check:** Review the prompt template — is the Decentralization Score passed as a fixed number, or does the prompt ask the LLM to evaluate and assign a score?

**Fix:** NEVER let the LLM decide the score. Calculate it deterministically, pass it as a fixed value, and only let the LLM generate the narrative around it.

---

## Most Likely Cause

Given that:
- The GVS and sub-scores (HPR, DEI, PDI, GPI) are **stable** across generations
- The Decentralization Score is the **only value that changes**
- The score **drifts downward** (not random — 4→3→3→3→2)
- The narrative tone also gets progressively more negative with each generation

**This strongly suggests Hypothesis 1 or 4** — the Decentralization Score is being generated or influenced by an LLM call with non-zero temperature, and the LLM is interpreting the same data differently each time.

---

## Required Fix

1. **Calculate the Decentralization Score deterministically** from the underlying metrics (Gini, Nakamoto, Top 5 concentration, etc.) using a fixed formula
2. **Pass the calculated score as a fixed input** to any LLM call that generates narrative text
3. **Never allow the LLM to assign or modify the numeric score**
4. **Log the raw score** for debugging: add a `decentralization_score_raw` field to the report metadata

## Acceptance Criteria

- [ ] Generate 5 reports for Lido in sequence — all must show the **exact same** Decentralization Score
- [ ] Generate a report, wait 1 hour, generate another — scores must match
- [ ] The Decentralization Score must be traceable to a specific formula applied to specific data points

---

## Decision (Feb 10, 2026)

**Remove the Decentralization Score entirely.** Replace with the GVS on the cover page.

**Rationale:**
- The "Decentralization Score" does not exist anywhere on the website
- It was being generated/interpreted by the LLM, not calculated deterministically
- The GVS (6.9 C) is the established, stable, deterministic score that matches the website
- Having a PDF-only score that contradicts the website destroys trust

**Implementation:**
1. Cover page: Replace "X/10 Decentralization Score" with "6.9 GVS" + letter grade (C) from the existing calculation
2. Page 2: Remove the "Decentralization Score X/10 (X)" header, use GVS instead
3. Remove the score explanation paragraph ("The Decentralization Score measures power distribution specifically...") — no longer needed since there's only one score
4. Keep the power distribution metrics (Gini, Nakamoto, Top 5%) as data points — they speak for themselves without a made-up aggregate score

**Status:** Instruction given to developer. Awaiting implementation.
